{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lithological Classification of Well Logs Using Machine Learning\n",
    "\n",
    "## 1. Project Overview\n",
    "This notebook implements a machine learning workflow to classify lithofacies from well log measurements using the FORCE 2020 dataset. The methodology employs **Gradient Boosting (XGBoost)**, enhanced by domain-specific feature engineering (petrophysical ratios) and vectorized data augmentation (spatial windowing).\n",
    "\n",
    "### Objectives\n",
    "- Load and preprocess well log data.\n",
    "- Engineer features based on rock physics principles.\n",
    "- Train an XGBoost classifier using GPU acceleration.\n",
    "- Evaluate model performance on a blind test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Library Import and Environment Setup\n",
    "The following cell initializes the Python environment by importing necessary libraries for data manipulation, visualization, and machine learning.\n",
    "\n",
    "**Technical Configuration:**\n",
    "* **Garbage Collection (`gc`):** Imported to explicitly manage memory during the iterative cross-validation process.\n",
    "* **Warning Suppression:** Enabled to maintain clean output logs during training.\n",
    "* **Visualization:** Seaborn is configured with the 'ticks' style for academic-grade plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 100)\n",
    "sns.set_style(\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Acquisition\n",
    "The dataset is loaded directly from the source repository. To ensure data integrity for validation, the test features are merged with their corresponding target labels (`test_target.csv`) using the unique well identifier and depth (`DEPTH_MD`).\n",
    "\n",
    "**Note:** The training set is large (~1.17M rows), making efficient memory management crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 1. Data Loading (Raw Data)\n",
    "# ==================================================================================\n",
    "print(\"â³ Loading datasets directly from source...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Load raw data directly to bypass previous bad imputation\n",
    "train = pd.read_csv('https://media.githubusercontent.com/media/Husayn01/Academic-Research/refs/heads/main/Data/train.csv', sep=';')\n",
    "test = pd.read_csv('https://media.githubusercontent.com/media/Husayn01/Academic-Research/refs/heads/main/Data/test_features.csv', sep=';')\n",
    "test_target = pd.read_csv('https://media.githubusercontent.com/media/Husayn01/Academic-Research/refs/heads/main/Data/test_target.csv', sep=';')\n",
    "\n",
    "# Merge test target\n",
    "test = test.merge(test_target, on=['WELL', 'DEPTH_MD'], how='left')\n",
    "\n",
    "print(f\"âœ… Data Loaded in {time.time() - start_time:.2f}s\")\n",
    "print(f\"   Train shape: {train.shape}\")\n",
    "print(f\"   Test shape: {test.shape}\")\n",
    "\n",
    "target_col = 'FORCE_2020_LITHOFACIES_LITHOLOGY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Strategic Feature Engineering\n",
    "Raw well logs often contain non-linear relationships or noise that hinder model performance. This section applies petrophysical transformations to extract meaningful geological signals.\n",
    "\n",
    "### Transformations Applied:\n",
    "1.  **Dimensionality Reduction:** Sparse columns (e.g., `SGR`, `RXO`, `ROPA`) are dropped to prevent the model from learning noise from missing values.\n",
    "2.  **Logarithmic Scaling:** Applied to resistivity logs (`RDEP`, `RMED`) because earth conductivity varies over multiple orders of magnitude.\n",
    "3.  **Geological Ratios:**\n",
    "    * **`GR_RHOB`:** Helps distinguish between shale volume (high GR) and compaction effects (high density).\n",
    "    * **Neutron-Density Separation (`NPHI - RHOB`):** A classic \"balloon effect\" indicator used to identify gas zones and differentiate sandstone from limestone.\n",
    "    * **Photoelectric Effect (`PEF^2`):** Squared to emphasize sensitivity to mineralogy (atomic number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 2. Strategic Feature Engineering\n",
    "# ==================================================================================\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # A. Drop sparse/noisy columns (Winning strategy from competition)\n",
    "    # SGR, ROPA, RXO etc. had too many missing values and introduced noise\n",
    "    drop_cols = ['SGR', 'ROPA', 'RXO', 'MUDWEIGHT', 'DCAL', 'RMIC', 'FORCE_2020_LITHOFACIES_CONFIDENCE']\n",
    "    df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "    # B. Log transformations for resistivity (Standard Petrophysics)\n",
    "    # Resistivity spans logarithmic scales (0.2 to 2000+ ohm.m)\n",
    "    for col in ['RDEP', 'RMED', 'RSHA']:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_LOG'] = np.log1p(df[col])\n",
    "\n",
    "    # C. Domain Ratios & Differences\n",
    "    # GR/RHOB: Shale indicator vs Compaction\n",
    "    if 'GR' in df.columns and 'RHOB' in df.columns:\n",
    "        df['GR_RHOB'] = df['GR'] / (df['RHOB'] + 0.001)\n",
    "\n",
    "    # Neutron-Density Separation (Gas Effect/Lithology)\n",
    "    if 'NPHI' in df.columns and 'RHOB' in df.columns:\n",
    "        # Approx matrix density 2.65 for Sandstone\n",
    "        df['NPHI_RHOB_DIFF'] = df['NPHI'] - (2.65 - df['RHOB'])\n",
    "\n",
    "    # Photoelectric Effect squared (Mineralogy)\n",
    "    if 'PEF' in df.columns:\n",
    "        df['PEF_SQ'] = df['PEF'] ** 2\n",
    "\n",
    "    # D. Fill MISSING categorical values string before encoding\n",
    "    # We do NOT impute numerical values. XGBoost handles NaNs natively.\n",
    "    if 'GROUP' in df.columns:\n",
    "        df['GROUP'] = df['GROUP'].fillna('Unknown')\n",
    "    if 'FORMATION' in df.columns:\n",
    "        df['FORMATION'] = df['FORMATION'].fillna('Unknown')\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"ðŸ› ï¸ Engineering base features...\")\n",
    "train_eng = engineer_features(train)\n",
    "test_eng = engineer_features(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vectorized Data Augmentation (Spatial Windowing)\n",
    "Geological data is spatially correlated; the lithology at depth $d$ is highly influenced by the layers at $d-1$ and $d+1$. We implement a **windowing strategy** to incorporate vertical context.\n",
    "\n",
    "**Implementation Detail:**\n",
    "Instead of slow iterative loops, we use vectorized `shift` operations grouped by `WELL` ID. This creates lag (previous) and lead (next) features, as well as gradient features (instantaneous change) to detect bed boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 3. Vectorized Augmentation (Windowing)\n",
    "# ==================================================================================\n",
    "def vectorized_windowing(df, cols, window_size=1):\n",
    "    \"\"\"Creates lag/lead features using vectorized shifts (faster than rolling).\"\"\"\n",
    "    df_out = df.copy()\n",
    "    # Sort to ensure depth continuity\n",
    "    df_out = df_out.sort_values(['WELL', 'DEPTH_MD'])\n",
    "\n",
    "    for col in cols:\n",
    "        if col not in df_out.columns:\n",
    "            continue\n",
    "        # Create lags (above) and leads (below)\n",
    "        for i in range(1, window_size + 1):\n",
    "            df_out[f'{col}_prev_{i}'] = df_out.groupby('WELL')[col].shift(i)\n",
    "            df_out[f'{col}_next_{i}'] = df_out.groupby('WELL')[col].shift(-i)\n",
    "        # Gradients (Instantaneous change)\n",
    "        df_out[f'{col}_grad'] = df_out[col] - df_out.groupby('WELL')[col].shift(1)\n",
    "\n",
    "    return df_out\n",
    "\n",
    "print(\"ðŸªŸ Applying window augmentation...\")\n",
    "aug_cols = ['GR', 'RHOB', 'NPHI', 'DTC', 'RDEP_LOG']\n",
    "train_final = vectorized_windowing(train_eng, aug_cols, window_size=1)\n",
    "test_final = vectorized_windowing(test_eng, aug_cols, window_size=1)\n",
    "\n",
    "print(f\"   Final Train shape: {train_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preprocessing and Encoding\n",
    "Before training, categorical variables are encoded into integer formats required by XGBoost. Additionally, column names are sanitized to remove special characters (like brackets `[]` common in LAS files) which can cause parsing errors in the model.\n",
    "\n",
    "**Steps:**\n",
    "1.  **Label Encoding:** `GROUP` and `FORMATION` columns are fitted on the combined dataset to handle potential unseen labels in the test set (a strategy often used in competitions, though technically a form of minor leakage; see review below).\n",
    "2.  **Target Encoding:** The lithology labels are transformed into a 0-N range.\n",
    "3.  **Feature Split:** Dropping non-predictive metadata (`DEPTH_MD`, `X_LOC`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 4. Encoding & Final Prep\n",
    "# ==================================================================================\n",
    "# Encode Categoricals\n",
    "cat_cols = ['GROUP', 'FORMATION', 'WELL']\n",
    "for col in cat_cols:\n",
    "    if col in train_final.columns:\n",
    "        le = LabelEncoder()\n",
    "        # Fit on both train and test to catch all categories\n",
    "        unique_vals = pd.concat([train_final[col], test_final[col]]).astype(str).unique()\n",
    "        le.fit(unique_vals)\n",
    "        train_final[col] = le.transform(train_final[col].astype(str))\n",
    "        test_final[col] = le.transform(test_final[col].astype(str))\n",
    "\n",
    "# Encode Target\n",
    "le_target = LabelEncoder()\n",
    "train_final[target_col] = le_target.fit_transform(train_final[target_col])\n",
    "test_final[target_col] = le_target.transform(test_final[target_col])\n",
    "\n",
    "# Define X and y\n",
    "drop_for_train = ['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', target_col]\n",
    "X = train_final.drop(columns=drop_for_train)\n",
    "y = train_final[target_col]\n",
    "X_test_data = test_final.drop(columns=drop_for_train)\n",
    "y_test_data = test_final[target_col]\n",
    "\n",
    "# Clean column names for XGBoost\n",
    "X.columns = [c.replace('[', '_').replace(']', '_') for c in X.columns]\n",
    "X_test_data.columns = [c.replace('[', '_').replace(']', '_') for c in X_test_data.columns]\n",
    "\n",
    "print(\"âœ¨ Features ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training: XGBoost with Stratified K-Fold\n",
    "The model is trained using **Stratified 10-Fold Cross-Validation**. Stratification is essential here due to the severe class imbalance in geological datasets (e.g., massive Shale intervals vs. thin Coal or Basement layers).\n",
    "\n",
    "**Model Parameters:**\n",
    "* `tree_method='hist'`, `device='cuda'`: Enables **GPU acceleration** for significantly faster training on large datasets.\n",
    "* `objective='multi:softprob'`: Used for multi-class classification to output probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# 5. Training with GPU (XGBoost 2.0+ Compatible)\n",
    "# ==================================================================================\n",
    "# Model Parameters (Optimized for GPU)\n",
    "xgb_params = {\n",
    "    'n_estimators': 300,\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'random_state': 42,\n",
    "    'objective': 'multi:softprob',\n",
    "    'early_stopping_rounds': 30,\n",
    "    'n_jobs': -1,\n",
    "    'reg_lambda': 1.5,\n",
    "    'reg_alpha': 0.5,\n",
    "    # --- CRITICAL GPU SETTINGS FOR XGBOOST 2.0+ ---\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda'\n",
    "}\n",
    "\n",
    "folds = 10\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "test_preds_accum = np.zeros((len(X_test_data), 12))\n",
    "scores = []\n",
    "\n",
    "print(f\"ðŸš€ Starting {folds}-Fold Cross-Validation on T4 GPU...\")\n",
    "cv_start = time.time()\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    fold_start = time.time()\n",
    "    X_fold_train, y_fold_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_fold_val, y_fold_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = XGBClassifier(**xgb_params)\n",
    "\n",
    "    model.fit(\n",
    "        X_fold_train, y_fold_train,\n",
    "        eval_set=[(X_fold_val, y_fold_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    val_probs = model.predict_proba(X_fold_val)\n",
    "    test_probs = model.predict_proba(X_test_data)\n",
    "    test_preds_accum += test_probs / folds\n",
    "\n",
    "    # Score\n",
    "    val_pred_labels = np.argmax(val_probs, axis=1)\n",
    "    acc = accuracy_score(y_fold_val, val_pred_labels)\n",
    "    scores.append(acc)\n",
    "\n",
    "    print(f\"   Fold {fold+1}/{folds} | Accuracy: {acc:.4f} | Time: {time.time() - fold_start:.1f}s\")\n",
    "\n",
    "    del X_fold_train, X_fold_val, model\n",
    "    gc.collect()\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(f\"âœ… Average CV Accuracy: {np.mean(scores):.4f}\")\n",
    "print(f\"â±ï¸ Total CV Time: {(time.time() - cv_start)/60:.2f} minutes\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# ==================================================================================\n",
    "# 6. Final Evaluation\n",
    "# ==================================================================================\n",
    "# Final Predictions\n",
    "final_test_labels = np.argmax(test_preds_accum, axis=1)\n",
    "\n",
    "# Metrics\n",
    "test_acc = accuracy_score(y_test_data, final_test_labels)\n",
    "test_f1 = f1_score(y_test_data, final_test_labels, average='weighted')\n",
    "\n",
    "print(f\"\\nðŸ† Final Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"ðŸ† Final Test F1 (Weighted): {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Evaluation and Visualization\n",
    "We convert the numeric predictions back to geological lithology names and generate a classification report. A confusion matrix is plotted to analyze misclassifications, particularly between geologically similar classes (e.g., Sandstone vs. Sandstone/Shale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Lithology mapping for readable labels\n",
    "lithology_keys = {30000: 'Sandstone', 65030: 'Sandstone/Shale', 65000: 'Shale',\n",
    "                  80000: 'Marl', 74000: 'Dolomite', 70000: 'Limestone',\n",
    "                  70032: 'Chalk', 88000: 'Halite', 86000: 'Anhydrite',\n",
    "                  99000: 'Tuff', 90000: 'Coal', 93000: 'Basement'}\n",
    "\n",
    "# Create friendly labels mapping original codes to names\n",
    "target_names_mapped = [lithology_keys.get(code, str(code)) for code in le_target.classes_]\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "# FIX: Provide explicit labels to handle missing classes in test set\n",
    "all_labels = np.arange(len(le_target.classes_))\n",
    "print(classification_report(\n",
    "    y_test_data,\n",
    "    final_test_labels,\n",
    "    labels=all_labels,\n",
    "    target_names=target_names_mapped,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# Optional: Confusion Matrix Plot\n",
    "try:\n",
    "    # FIX: Pass labels=all_labels to ensure the matrix shape matches the target names,\n",
    "    # even if some classes are missing in y_test_data.\n",
    "    cm = confusion_matrix(y_test_data, final_test_labels, labels=all_labels)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=target_names_mapped,\n",
    "                yticklabels=target_names_mapped)\n",
    "    plt.title(f'Confusion Matrix (Acc:)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot confusion matrix: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Technical Review & Conclusion\n",
    "\n",
    "### Model Behavior\n",
    "* **Generalization:** The discrepancy between CV accuracy (~93%) and Test accuracy (~78%) suggests a **distribution shift** between the training wells and the blind test wells. This is common in subsurface data where geological trends vary spatially (e.g., North vs. South North Sea).\n",
    "* **Class Imbalance:** High-frequency classes (Shale, Sandstone) perform well (F1 > 0.8). Rare classes (Dolomite, Coal, Basement) suffer from low recall (F1 ~ 0.0).\n",
    "\n",
    "### Identified Issues & Recommendations\n",
    "* **Geographic Overfitting:** Including `X_LOC` and `Y_LOC` often causes trees to overfit to specific coordinates rather than learning rock physics. **Recommendation:** Experiment with removing coordinates or replacing them with basin-relative trends.\n",
    "* **Label Leakage:** Fitting the `LabelEncoder` on the combined Train+Test set (`pd.concat`) allows the model to anticipate categories in the test set. While harmless for label mapping, ensuring the model can handle \"Unknown\" categories during inference is critical for production.\n",
    "* **Mixed Lithologies:** The class \"Sandstone/Shale\" has low recall (often confused with pure Sandstone or Shale). This represents a physical ambiguity in the logs that standard trees struggle to resolve without higher-resolution data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}